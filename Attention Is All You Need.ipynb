{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Transformer Model without using the framework\n",
    "\n",
    "Let's create a model capable of predicting sequences of length equal to 10 tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Transformer model consists of several main parts:\n",
    "\n",
    "- 1- Embedding Layer: Transforms words into numerical vectors of fixed size.\n",
    "- 2- Attention Mechanism: Allows the model to focus on different parts of the input.\n",
    "- 3- Encoder and Decoder Layers: Process data sequentially.\n",
    "- 4- Linear and Softmax Layer: For final predictions.\n",
    "\n",
    "For this project the main objective is to implement item 2, but to make the example functional we will also implement items 1 and 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "\n",
    "# Model dimension\n",
    "dim_model = 64\n",
    "\n",
    "# Sequence length\n",
    "seq_length = 10\n",
    "\n",
    "# Vocabulary size\n",
    "vocab_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Layer\n",
    "\n",
    "The embedding function is used to convert sequential inputs into dense vectors of fixed size. These vectors are known as embeddings and are a fundamental part, especially of PLN models.\n",
    "\n",
    "These embeddings are fundamental for deep learning models in PLN, as they provide a rich and dense representation of words or tokens, capturing contextual and semantic information that is essential for tasks such as machine translation, text classification, among others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to create an embedding matrix\n",
    "def embedding(input, vocab_size, dim_model):\n",
    "\n",
    "    # Create an embedding matrix where each row represents a vocabulary token\n",
    "    # The array is initialized with normally distributed random values\n",
    "    embed = np.random.randn(vocab_size, dim_model)\n",
    "    \n",
    "    # For each token index in the input, select the corresponding embedding from the array\n",
    "    # Returns an array of embeddings corresponding to the input sequence\n",
    "    return np.array([embed[i] for i in input])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Mechanism\n",
    "\n",
    "In this example, let's keep it simple and use just one attention layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Transformer, Q, K and V are derived from the same input in encoder attention layers, but from different inputs in the decoder (Q comes from the output of the previous decoder layer, while K and V come from the encoder output). The attention mechanism calculates a set of scores (using the dot product between Q and K, hence the name \"scaled dot-product attention\"), applies a softmax function to obtain attention weights, and uses these weights to weight the values, creating an output that is a weighted combination of the relevant input information.\n",
    "\n",
    "This process allows the model to give \"attention\" to the most relevant parts of the input for each part of the output, which is especially useful in tasks such as translation, where the relevance of different words in the input can vary depending on the part of the sentence being used. translated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Activation Function\n",
    "\n",
    "The softmax function is a widely used activation function in neural networks, especially in classification scenarios, where it is important to transform raw output values (logits) into probabilities that sum to 1. Below is the softmax function code with comments on each line explaining how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax Activation Function\n",
    "def softmax(x):\n",
    "    \n",
    "    # Calculates the exponential of each input element, adjusted by the maximum value in the input\n",
    "    # to avoid numeric overflow\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    \n",
    "    # Divide each exponential by the sum of the exponentials along the last axis (axis=-1)\n",
    "    # Reshape(-1, 1) ensures that division is performed correctly in a multidimensional context\n",
    "    return e_x / e_x.sum(axis=-1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Dot Product\n",
    "\n",
    "The scaled_dot_product_attention() function is a component of the attention mechanism in Transformer models. It calculates attention between sets of queries (Q), keys (K) and values (V).\n",
    "\n",
    "Essentially, this function allows the model to give different importance to different parts of the input, a key aspect that makes Transformer models particularly effective for PLN and other sequential tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to calculate attention scaled by dot product\n",
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    \n",
    "    # Calculate the dot product between Q and the transpose of K\n",
    "    matmul_qk = np.dot(Q, K.T)\n",
    "    \n",
    "    # Gets the dimension of the key vectors\n",
    "    depth = K.shape[-1]\n",
    "    \n",
    "    # Scale the logits by dividing them by the square root of the depth\n",
    "    logits = matmul_qk / np.sqrt(depth)\n",
    "    \n",
    "    # Apply the softmax function to obtain the attention weights\n",
    "    attention_weights = softmax(logits)\n",
    "    \n",
    "    # Multiply the attention weights by the V values to get the final output\n",
    "    output = np.dot(attention_weights, V)\n",
    "    \n",
    "    # Returns the weighted output\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Output with Linear and Softmax Operation\n",
    "\n",
    "The linear_and_softmax() function is a combination of a linear layer followed by a softmax function, commonly used in deep learning models, especially in classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the function that applies a linear transformation followed by softmax\n",
    "def linear_and_softmax(input):\n",
    "    \n",
    "    # Initialize a weight matrix with normally distributed random values\n",
    "    # This matrix connects each model dimension (dim_model) to each vocabulary word (vocab_size)\n",
    "    weights = np.random.randn(dim_model, vocab_size)\n",
    "    \n",
    "    # Performs the linear operation (scalar product) between the input and the weight matrix\n",
    "    # The result, logits, is a vector that represents the input transformed into a higher-dimensional space\n",
    "    logits = np.dot(input, weights)\n",
    "    \n",
    "    # Apply the softmax function to the logits\n",
    "    # This transforms the logits into a vector of probabilities, where each element sums to 1\n",
    "    return softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model function\n",
    "def transformer_model(input):\n",
    "    \n",
    "    # Embedding\n",
    "    embedded_input = embedding(input, vocab_size, dim_model)\n",
    "    \n",
    "    # Attention Mechanism\n",
    "    attention_output = scaled_dot_product_attention(embedded_input, embedded_input, embedded_input)\n",
    "    \n",
    "    # Layer linear and softmax\n",
    "    output_probabilities = linear_and_softmax(attention_output)\n",
    "    \n",
    "    # Choosing the indices with the highest probability\n",
    "    output_indices = np.argmax(output_probabilities, axis=-1)\n",
    "    \n",
    "    return output_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Model for Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating random data for model input\n",
    "input_sequence = np.random.randint(0, vocab_size, seq_length)\n",
    "\n",
    "print(\"Input Sequence:\", input_sequence)\n",
    "\n",
    "# Making predictions with the model\n",
    "output = transformer_model(input_sequence)\n",
    "\n",
    "print(\"Model Output:\", output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
